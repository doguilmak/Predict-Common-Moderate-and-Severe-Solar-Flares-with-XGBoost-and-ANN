# -*- coding: utf-8 -*-
"""pred_solar_flare_class_ann.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kv2_324i8lNwfE96_e_pRNdG3xjd9Ojr

<h1  align=center><font  size = 6>Predicting Solar Flares and Flare Area Classes</font></h1>

<p align="center">
    <img src="https://images.ctfassets.net/cnu0m8re1exe/1RiWk3i0ceklxImgCFDrTD/af90389b5fadb31b7142b9f98247f917/Untitled_design__54_.png?fm=jpg&fl=progressive&w=660&h=433&fit=fill" height=400  width=1000> 
</p>

<small>Picture Source:<a  href="https://images.ctfassets.net/cnu0m8re1exe/1RiWk3i0ceklxImgCFDrTD/af90389b5fadb31b7142b9f98247f917/Untitled_design__54_.png?fm=jpg&fl=progressive&w=660&h=433&fit=fill">ctfassets.net</a></small>

<br>

<h2>Statement</h2>

<p>The purpose of this study is based on the available data, it was estimated numbers of the <i>solar flares</i> production in spesific region on sun in the following 24 hours (with classes). You can run, modify and download your own model from codes. Accuracy can change due to parameters. Please let me know the best parameters.</p>

<br>

<h2>Keywords</h2>
<ul>
  <li>Multi-Output</li>
  <li>Neural Networks</li>
  <li>Space</li>
  <li>Sun</li>
  <li>Classification</li>
	<li>Deep Learning</li>
</ul>

<br>

<h2>Datasets</h2>

<p>Datasets are downloaded from <a href="https://archive.ics.uci.edu/ml/datasets/Solar+Flare">archive.ics.uci.edu</a> website. You can find the details of the datasets in that website and also in the <i>flare.names</i> named file. <i>flare.data1</i> dataset has <i>13 columns</i> and <i>323 rows without the header</i> and <i>flare.data2</i> dataset has <i>13 columns</i> and <i>1066 rows without the header</i>. The database contains 3 potential classes, one for the number of times a certain type of solar flare occured in a 24 hour period. Each instance represents captured features for 1 active region on the sun. The data are divided into two sections. <b>The second section (flare.data2) has had much more error correction applied to the it, and has consequently been treated as more reliable.</b></p>

<br>

<h2>Table of Contents</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#import">Import Libraries</a></li>
<li><a href="https://#data_preparation">Dataset Preparation</a></li>
<li><a href="https://#compile_fit">Build and Fit the Model</a></li>

<br>

<p></p>
Estimated Time Needed: <strong>15 min</strong>
</div>

<br>
<h2 align=center id="import">Import Libraries</h2>
<p>The following are the libraries we are going to use for this lab:</p>
"""

try:
  # %tensorflow_version only exists in Colab.
  get_ipython().run_line_magic('tensorflow_version', '2.x')
except Exception:
  pass

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")

"""<br>
<h2 align=center id="data_preparation">Dataset Preparation (Data Preprocessing)</h2>

<p>Let's build some necessary functions for visualisation and pre-processing.</p>

"""

# Functions taken from 'Custom Models, Layers, and Loss Functions with TensorFlow' course.
# https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow

def format_output(data):
    y1 = data.pop('c_class')
    y1 = np.array(y1)
    y2 = data.pop('m_class')
    y2 = np.array(y2)
    y3 = data.pop('x_class')
    y3 = np.array(y3)
    return y1, y2, y3


def norm(x):
    return (x - train_stats['mean']) / train_stats['std']

def plot_metrics(metric_name, title, ylim=5):
    plt.figure(figsize = (20, 10))
    sns.set_style('whitegrid')
    plt.title(title)
    plt.ylim(0, ylim)
    plt.plot(history.history[metric_name], color='blue', label=metric_name)
    plt.plot(history.history['val_' + metric_name], color='green', label='val_' + metric_name)
    plt.legend(['train', 'validation'])
    plt.show()

"""<p>Now, we can upload our data named <code>flare.data2</code>. It doesn't have named columns. We need to spesify them after importing our data into <i>dataframe</i>.</p>"""

df = pd.read_csv('/content/flare.data2', sep=" ", skiprows=1, header=None)
df.columns = ['class', 'largest_spot_size', 'spot_dist', 'activity', 'evolution', 'flare_activity_code', 'h_complex', 'h_complex_sdisk', 'area', 'area_largest_spot', 'c_class', 'm_class', 'x_class']

df.shape

"""<b>Attribute Information:</b>

<ol>
	<li>Code for class <i>(modified Zurich class)</i> <i>(A,B,C,D,E,F,H)</i></li>
	<li>Code for largest spot size <i>(X,R,S,A,H,K)</i></li>
	<li>Code for spot distribution <i>(X,O,I,C)</i></li>  
	<li>Activity <i>(1 = reduced, 2 = unchanged)</i></li>  
	<li>Evolution <i>(1 = decay, 2 = no growth, 3 = growth)</i></li>  
	<li>Previous 24 hour flare activity code <i>(1 = nothing as big as an M1, 2 = one M1, 3 = more activity than one M1)</i></li>  
	<li>Historically-complex <i>(1 = Yes, 2 = No)</i></li>  
	<li>Did region become historically complex on this pass across the sun's disk <i>(1 = yes, 2 = no)</i></li>  
	<li>Area <i>(1 = small, 2 = large)</i></li>  
	<li>Area of the largest spot <i>(1 = <=5, 2 = >5)</i></li>  
</ol>

<b>From all these predictors three classes of flares are predicted, which are represented in the last three columns.</b> 

11. <i>C-class flares</i> production by this region in the following 24 hours (common flares)
12. <i>M-class flares</i> production by this region in the following 24 hours (moderate flares) 
13. <i>X-class flares</i> production by this region in the following 24 hours (severe flares)
"""

print("Number of NaN values: {}.".format(df.isnull().sum().sum()))

print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

"""<p>We have 701 duplicated rows. We need to drop them because we want the program to learn, not memorize.</p>"""

dp = df[df.duplicated(keep=False)]
df.drop_duplicates(inplace= True)
print("Number of duplicated rows: {}.".format(df.duplicated().sum()))

df.describe().T

"""<p>As we can see from the info graph, <code>area_largest_spot</code> has only one unique value. This column did not affects our prediction so we can drop it. </p>"""

df['area_largest_spot'].unique()

df.drop(['area_largest_spot'], axis = 1, inplace = True)

df.head()

df.tail()

df.info()

df.shape

df_copy = df.copy()

le = LabelEncoder()
df["class"] = le.fit_transform(df['class'])
df["largest_spot_size"] = le.fit_transform(df['largest_spot_size'])
df["spot_dist"] = le.fit_transform(df['spot_dist'])

"""<p>Let's see the difference between label encoded columns:</p>"""

df[['class', 'largest_spot_size', 'spot_dist']].head(10)

df_copy[['class', 'largest_spot_size', 'spot_dist']].head(10)

plt.figure(figsize = (20, 20))
sns.heatmap(df.corr(), annot=True)

plt.figure(figsize = (20, 20))
sns.pairplot(df)

"""<p>Split the data into train and test with 80 train / 20 test:</p>"""

train, test = train_test_split(df, test_size=0.2)
train_stats = train.describe()

train_stats.pop('c_class')
train_stats.pop('m_class')
train_stats.pop('x_class')
train_stats = train_stats.transpose()
train_Y = format_output(train)
test_Y = format_output(test)

train_Y

test_Y

# Normalize the training and test data
norm_train_X = norm(train)
norm_test_X = norm(test)

"""<br>
<h2 align=center id="compile_fit">Build and Fit the Model</h2>
"""

def create_model(input_len):
  input_layer = Input(shape=(len(train.columns), ))
  first_dense = Dense(units='128', activation='relu')(input_layer)
  second_dense = Dense(units='128', activation='relu')(first_dense)
  third_dense = Dense(units='64', activation='relu')(second_dense)

  fourth_dense = Dense(units='32', activation='sigmoid')(third_dense)
  y1_output = Dense(units='1', name='c_output')(fourth_dense)

  fifth_dense = Dense(units='64', activation='sigmoid')(third_dense)
  y2_output = Dense(units='1', name='m_output')(fifth_dense)

  sixth_dense = Dense(units='16', activation='sigmoid')(third_dense)
  y3_output = Dense(units='1', name='x_output')(sixth_dense)

  model = Model(inputs=input_layer, outputs=[y1_output, y2_output, y3_output])

  print(model.summary())
  return model

model = create_model(train.columns)
model

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')

"""<p>Let's configure parameters</p>"""

optimizer = tf.keras.optimizers.SGD(lr=0.001)
model.compile(optimizer=optimizer,
              loss={'c_output': 'mse', 'm_output': 'mse', 'x_output': 'mse'},
              metrics={'c_output': tf.keras.metrics.RootMeanSquaredError(),
                       'm_output': tf.keras.metrics.RootMeanSquaredError(),
                       'x_output': tf.keras.metrics.RootMeanSquaredError()})

"""<p>It is time to train our model!</p>"""

EPOCHS = 500 #@param {type:"number"}
BATCH_SIZE = 10 #@param {type:"number"}

history = model.fit(norm_train_X, train_Y,
                    epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(norm_test_X, test_Y))

loss, C_loss, M_loss, X_loss, C_rmse, M_rmse, X_rmse = model.evaluate(x=norm_test_X, y=test_Y)
print("Loss = {}, c_output_loss = {}, c_output_mse = {}, m_output_loss = {}, m_output_mse = {}, x_output_loss = {}, x_output_mse = {}".format(loss, C_loss, C_rmse, M_loss, M_rmse, X_loss, X_rmse))

plot_metrics(metric_name='c_output_root_mean_squared_error', title='C RMSE', ylim=2)
plot_metrics(metric_name='m_output_root_mean_squared_error', title='M RMSE', ylim=2)
plot_metrics(metric_name='x_output_root_mean_squared_error', title='X RMSE', ylim=2)

!mkdir -p saved_model
model.save('saved_model/my_model')

model.save('my_model.h5')

"""<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>

"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")